import os
from typing import Literal
from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import PydanticOutputParser

# ==========================================
# 1. å®šä¹‰æ•°æ®ç»“æ„ (Pydantic Model)
# ==========================================
class CustomerResponse(BaseModel):
    thought_process: str = Field(
        description="The internal reasoning process (Chain of Thought). CRITICAL: Analyze the salesperson's offer against market price and your persona before speaking."
    )
    spoken_response: str = Field(
        description="The actual words spoken to the salesperson. Keep it natural and consistent with your persona."
    )
    status: Literal["CONTINUE", "DEAL", "LEAVE"] = Field(
        description="Current conversation status. 'DEAL' if you buy, 'LEAVE' if you leave, 'CONTINUE' otherwise."
    )

# ==========================================
# 2. LLM åˆå§‹åŒ–
# ==========================================
def get_customer_llm():
    """
    è·å– LLM å®ä¾‹ã€‚è‡ªåŠ¨è¯»å– .env ä¸­çš„é…ç½®ã€‚
    é’ˆå¯¹ SiliconFlow è¿›è¡Œäº†ä¼˜åŒ–ã€‚
    """
    # å¦‚æœ .env é…ç½®äº† SiliconFlowï¼Œè¿™é‡Œä¼šè‡ªåŠ¨ä½¿ç”¨
    # å»ºè®®ä½¿ç”¨ Qwen2.5-72B-Instruct æˆ– DeepSeek-V3ï¼Œæ¨ç†èƒ½åŠ›å¼º
    return ChatOpenAI(
        model="Qwen/Qwen2.5-72B-Instruct",  # ä¹Ÿå¯ä»¥æ¢æˆ "deepseek-ai/DeepSeek-V3"
        temperature=0.6, # æ¸©åº¦é€‚ä¸­ï¼Œå¹³è¡¡åˆ›é€ æ€§ä¸æŒ‡ä»¤éµå¾ªèƒ½åŠ›
        max_tokens=1024
    )

# ==========================================
# 3. æ ¸å¿ƒç”Ÿæˆå‡½æ•° (The Brain)
# ==========================================
def generate_customer_response(history, persona: str, stage: str, context: str) -> CustomerResponse:
    """
    æ ¸å¿ƒé€»è¾‘ï¼šæ ¹æ®å†å²å¯¹è¯ã€äººè®¾ã€é˜¶æ®µå’Œ RAG çŸ¥è¯†ç”Ÿæˆå›å¤ã€‚
    """
    llm = get_customer_llm()
    
    # ä½¿ç”¨ Parser ç¡®ä¿è¾“å‡ºæ ¼å¼ç¨³å®š
    parser = PydanticOutputParser(pydantic_object=CustomerResponse)

    # --- System Prompt è®¾è®¡ (æ ¸å¿ƒè€ƒå¯Ÿç‚¹) ---
    # é‡‡ç”¨äº† "Role-Playing" + "Context-Injection" + "CoT-Enforcement"
    system_prompt_template = """
You are a virtual customer in a jewelry store simulating a real-world sales scenario.

=== ğŸ­ YOUR PERSONA: {persona} ===
- **Budget Sensitive**: You verify every price against market data. If > 600 RMB/g, you complain. You care about labor costs.
- **Unique Design**: You dislike common styles (like plain glossy). You want "Gu Fa Jin" (Ancient Method) or enamel. Price is secondary.
- **Indecisive**: You are easily swayed but hard to close. You always ask "What do you think?" or "Let me compare".

=== ğŸ“Š SALES STAGE: {stage} ===
(Needs Analysis -> Product Recommendation -> Objection Handling -> Closing)

=== ğŸ“š MARKET KNOWLEDGE (RAG Context) ===
Use this data to fact-check the salesperson. Do NOT hallucinate prices.
{context}

=== ğŸ§  INSTRUCTIONS (Chain of Thought) ===
1. **CRITICAL THINKING**: Before generating a response, you MUST think internally:
   - Does the salesperson's offer match the market price provided in the context?
   - Does the product match my persona's taste?
   - Is the salesperson being pushy?
2. **DECISION**:
   - If they offer a good deal or answer your concern perfectly -> DEAL.
   - If they are rude, price is too high, or product is wrong -> LEAVE.
   - Otherwise -> CONTINUE.
3. **OUTPUT**:
   - Generate the JSON response strictly following the format below.

{format_instructions}
"""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt_template),
        ("placeholder", "{messages}"),
    ])

    # å°†æ ¼å¼è¯´æ˜æ³¨å…¥ Prompt
    partial_prompt = prompt.partial(format_instructions=parser.get_format_instructions())

    # æ„å»ºè°ƒç”¨é“¾
    chain = partial_prompt | llm | parser
    
    try:
        # æ‰§è¡Œæ¨ç†
        response = chain.invoke({
            "persona": persona,
            "stage": stage, # å®é™…é¡¹ç›®ä¸­è¿™ä¸ª Stage å¯ä»¥ç”±å¦ä¸€ä¸ª Chain åŠ¨æ€åˆ¤æ–­
            "context": context,
            "messages": history
        })
        return response

    except Exception as e:
        print(f"âŒ [Customer Brain] Generation Error: {e}")
        # å…œåº•æœºåˆ¶ï¼šé˜²æ­¢ LLMå¶å°”æŠ½é£å¯¼è‡´ç¨‹åºå´©æºƒ
        return CustomerResponse(
            thought_process=f"Error during reasoning: {str(e)}. I should ask for clarification.",
            spoken_response="ä¸å¥½æ„æ€ï¼Œæˆ‘åˆšæ‰èµ°ç¥äº†ï¼Œæ‚¨èƒ½å†è¯´ä¸€éå—ï¼Ÿ",
            status="CONTINUE"
        )